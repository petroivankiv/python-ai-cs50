# Нім (гра)

Створіть штучний інтелект, який гратиме в «Нім» за допомогою навчання з підкріпленням. 

## Створіть штучний інтелект, який прогнозуватиме, чи завершать покупці купівлю товарів на сайті.

```
$ python play.py
Playing training game 1
Playing training game 2
Playing training game 3
...
Playing training game 9999
Playing training game 10000
Done training

Piles:
Pile 0: 1
Pile 1: 3
Pile 2: 5
Pile 3: 7

AI's Turn
AI chose to take 1 from pile 2.
```

## Технічні вимоги

Завершіть реалізацію функцій get_q_value, update_q_value, best_future_reward, та choose_action у nim.py. Для кожної з цих функцій щоразу, коли функція приймає стан (state), можна вважати його списком цілих чисел. Водночас щоразу, коли функція приймає дію (action), допустимо опрацьовувати її як пару цілих чисел (i, j) — купку i та кількість предметів j.

Функція *get_q_value* має приймати стан та дію і повертати відповідне значення Q для цієї пари стан/дія.

Функція *update_q_value* приймає стан state, дію action, відоме значення Q old_q, поточну винагороду reward та оцінку майбутньої винагороди future_rewards, після чого оновлює значення Q для пари стан/дія відповідно до формули Q-навчання.

Функція *best_future_reward* приймає стан і повертає найкращу можливу винагороду за будь-яку доступну дію в цьому стані, відповідно до даних в self.q.

Функція *choose_action* повинна приймати стан (і, за бажанням, прапорець епсилон (epsilon), якщо хочете використати епсилон-евристичний алгоритм), та повертати доступну дію у цьому стані.